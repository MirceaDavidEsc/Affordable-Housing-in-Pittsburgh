data.set<-read.csv('F:/DataAnalyticsClub/DACaseComp/property_assessments_clean.csv',stringsAsFactors=FALSE )
data.set[is.na(data.set)] <- 0
data.set[data.set == ""] <- 0

factorCols <- c("SCHOOLDESC","MUNIDESC","NEIGHDESC","TAXDESC","OWNERDESC","USEDESC","CLEANGREEN","FARMSTEADFLAG","COUNTYEXEMPTBLDG","STYLEDESC","EXTFINISH_DESC","ROOFDESC","BASEMENTDESC","GRADE","CDU","CONDITIONDESC","HEATINGCOOLINGDESC","BSMTGARAGE","HOMESTEADFLAG")
nonFactorCols<-c("PRICE","COUNTYTOTAL","LOCALTOTAL","FAIRMARKETTOTAL","STORIES","YEARBLT", "LOTAREA", "TOTALROOMS","BEDROOMS","FULLBATHS","HALFBATHS","FIREPLACES","FINISHEDLIVINGAREA")

data.set[factorCols] <- lapply(data.set[factorCols], factor)


#prepare date for train\test split

library(ROCR)

n.row <- nrow(data.set)                  # Number of rows in the data set
n.col <- ncol(data.set) 				 # Index of dependent variable 
                                         # Always last variable in the data set
data.set <- data.set[sample(n.row),]     # Shuffle the data set, change the order of the rows, 
                                         # Remove possible bias in data selection
train.rows <- I( round(n.row * (4/5) ) ) # 80% data for training

# Divide the data into training and testing
train <- data.set[0:train.rows , ]
test <- data.set[I(train.rows + 1) : n.row , ]
formula <- paste(colnames(data.set[n.col])," ~ .")


Xtrain<-train[ , -which(names(train) %in% c("PREVSALEDATE","PREVSALEDATE2","ClassLabel", "PARID", "PROPERTYUNIT", "PriceDiff1", "PriceDiff2", "DateDiff1", "DateDiff2", "SALEDATE"))]
Ytrain<-train[,'ClassLabel']


Xtest<-test[ , -which(names(test) %in% c("PREVSALEDATE","PREVSALEDATE2","ClassLabel", "PARID", "PROPERTYUNIT", "PriceDiff1", "PriceDiff2", "DateDiff1", "DateDiff2", "SALEDATE"))]
Ytest<-test[,'ClassLabel']


#go for boosting
#dropped some stuff related to boruta feature selection
library(gbm)
gbm1 <- gbm(ClassLabel  ~ . - PROPERTYZIP - MUNIDESC - COUNTYEXEMPTBLDG - FARMSTEADFLAG - TAXDESC - CLEANGREEN - OWNERDESC - BASEMENTDESC - STORIES - ROOFDESC - BSMTGARAGE - COOLINGDESC,
                        data = train, n.trees=3000, distribution="poisson", interaction.depth = 3)
best.iter <- gbm.perf(gbm1,method="OOB")

print(best.iter)
						
gbm.pred <- predict(gbm1,
                    test[, !(colnames(test) %in% c(test$PROPERTYZIP, test$MUNIDESC, test$COUNTYEXEMPTBLDG, test$FARMSTEADFLAG, test$TAXDESC, test$CLEANGREEN, test$OWNERDESC, test$BASEMENTDESC, test$STORIES, test$ROOFDESC, test$BSMTGARAGE, test$COOLINGDESC))], 
                    type = "response", n.trees = best.iter)
					


gbm.roc <- prediction(gbm.pred , test["ClassLabel"])
gbm.auc <- performance(gbm.roc, "auc")
gbm.perf <- performance(gbm.roc, measure = "tpr", x.measure = "fpr")
gbm.auc@y.name
gbm.auc@y.values
plot(gbm.perf)