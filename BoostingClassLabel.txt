data.set<-read.csv('F:/DataAnalyticsClub/DACaseComp/property_assessments_clean.csv',stringsAsFactors=FALSE )
data.set[is.na(data.set)] <- 0
data.set[data.set == ""] <- 0

factorCols <- c("SCHOOLDESC","MUNIDESC","NEIGHDESC","TAXDESC","OWNERDESC","USEDESC","CLEANGREEN","FARMSTEADFLAG","COUNTYEXEMPTBLDG","STYLEDESC","EXTFINISH_DESC","ROOFDESC","BASEMENTDESC","GRADE","CDU","CONDITIONDESC","HEATINGCOOLINGDESC","BSMTGARAGE","HOMESTEADFLAG")
nonFactorCols<-c("PRICE","COUNTYTOTAL","LOCALTOTAL","FAIRMARKETTOTAL","STORIES","YEARBLT", "LOTAREA", "TOTALROOMS","BEDROOMS","FULLBATHS","HALFBATHS","FIREPLACES","FINISHEDLIVINGAREA")

data.set[factorCols] <- lapply(data.set[factorCols], factor)


#prepare date for train\test split

library(ROCR)

n.row <- nrow(data.set)                  # Number of rows in the data set
n.col <- ncol(data.set) 				 # Index of dependent variable 
                                         # Always last variable in the data set
data.set <- data.set[sample(n.row),]     # Shuffle the data set, change the order of the rows, 
                                         # Remove possible bias in data selection
train.rows <- I( round(n.row * (4/5) ) ) # 80% data for training

# Divide the data into training and testing
train <- data.set[0:train.rows , ]
test <- data.set[I(train.rows + 1) : n.row , ]
formula <- paste(colnames(data.set[n.col])," ~ .")


Xtrain<-train[ , -which(names(train) %in% c("PREVSALEDATE","PREVSALEDATE2","ClassLabel", "PARID", "PROPERTYUNIT", "PriceDiff1", "PriceDiff2", "DateDiff1", "DateDiff2", "SALEDATE"))]
Ytrain<-train[,'ClassLabel']


Xtest<-test[ , -which(names(test) %in% c("PREVSALEDATE","PREVSALEDATE2","ClassLabel", "PARID", "PROPERTYUNIT", "PriceDiff1", "PriceDiff2", "DateDiff1", "DateDiff2", "SALEDATE"))]
Ytest<-test[,'ClassLabel']



#go for boosting
library(gbm)
gbm1 <- gbm(ClassLabel  ~ . - PROPERTYZIP - MUNIDESC - COUNTYEXEMPTBLDG,
                        data = train, n.trees=2000, distribution="poisson", interaction.depth = 3)
best.iter <- gbm.perf(gbm1,method="OOB")

print(best.iter)
						
gbm.pred <- predict(gbm1,
                    test[, !(colnames(test) %in% c(test$PROPERTYZIP, test$MUNIDESC, test$COUNTYEXEMPTBLDG))], 
                    type = "response", n.trees = best.iter)
					


gbm.roc <- prediction(gbm.pred , test["ClassLabel"])
gbm.auc <- performance(gbm.roc, "auc")
gbm.perf <- performance(gbm.roc, measure = "tpr", x.measure = "fpr")
gbm.auc@y.name
gbm.auc@y.values
plot(gbm.perf)



####################### DECISION TREES #############################################

############# rpart #############
library(rpart)

rpr.fit <- rpart(ClassLabel  ~ . - FARMSTEADFLAG - COUNTYEXEMPTBLDG - TAXDESC - CLEANGREEN - isVacant, data = train)

printcp(rpr.fit)
plot(rpr.fit)                                       # Print the classification tree
text(rpr.fit, pretty = TRUE)                        # print he labels of the tree

# Evaluate performance of the model
rpr.pred <- predict(rpr.fit, test)                  # Fetch predicted values from the model
rpr.roc <- prediction(rpr.pred[,2] , test[,n.col]) # Prediction object for ROCR
rpr.auc <- performance(rpr.roc, "auc")             # Actual AUC value
rpr.perf <- performance(rpr.roc, measure = "tpr", x.measure = "fpr") # For the plot
rpr.auc@y.name                                      # Print the AUC
rpr.auc@y.values                                    # Print the value of the AUC
plot(rpr.perf)                                      # Plot the AUC curve
